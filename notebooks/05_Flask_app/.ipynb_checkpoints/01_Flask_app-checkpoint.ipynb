{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e24380e-126d-4021-9490-0e24377426d0",
   "metadata": {},
   "source": [
    "# Falsk App\n",
    "- The purpose of htis  notebook is to perform POC to deploy a model on on Flask server , which runs locally on the machine.\n",
    "- Once this works, the same script will be made into app.py and deploy it on to AWS EC2 server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b45d84-1e93-488c-a965-4ff4986333cd",
   "metadata": {},
   "source": [
    "## Staley 16 Logistic Regression Model Flask Endpoint (POC)<a id=\"st16\">\n",
    "Load the ST16 Logistic Regression Model and expose as flask app end point "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e30230d-4073-4ab2-a025-f44f3e3edd5f",
   "metadata": {},
   "source": [
    "- Below cell reads the logisticRegressionModel.pkl file, which is a trained ST16 Model LR file.We use this model to make prediction \n",
    "- Once the below cell is run, the app will be serving on localhost:5000 port\n",
    "- Afer succesfull execution of the below cell, we can test this by pasting below URL in the browser http://127.0.0.1:5000/predict/?rainFall=20.4&siteId=2\n",
    "- Here siteId, is the used to identiy a particular record from ST16 data and the rainFall will be used as i15 parameter\n",
    "- we can change the 2 parameters to test the predict function\n",
    "- we can read the config data ( parameters ) from config.json file\n",
    "\n",
    "- Note : to stop the app running in jupyter notebook, you can either press \"Interrupt Kernel\" in Kernel from menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad386874-9103-40e6-88c7-9af2e369c215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhanu/anaconda3/envs/venv_debris_flow/lib/python3.9/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.10.3-CAPI-1.16.1) is incompatible with the GEOS version PyGEOS was compiled with (3.10.4-CAPI-1.16.2). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [03/Jun/2023 18:04:26] \"GET /predict/?rainFall=20.4&siteId=2 HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "seed=27\n",
    "from flask import Flask,request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import geopandas as gpd\n",
    "import sklearn\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, RocCurveDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "app = Flask(__name__)\n",
    "\n",
    "#read parameters from config file\n",
    "with open(\"config.json\",\"r\") as jsonfile:\n",
    "    config = json.load(jsonfile)\n",
    "\n",
    "#read data into pandas df\n",
    "sdf = pd.read_csv(config[\"data\"][\"lr_data_file\"])\n",
    "\n",
    "# add ID column , which can be used to identity a paritculay record\n",
    "sdf['ID'] = np.arange(sdf.shape[0])\n",
    "\n",
    "#load ST16 LR Model from pickle file \n",
    "with open(config[\"model\"][\"lr_pickle_file\"],'rb') as file:\n",
    "    lr_model = pickle.load(file)\n",
    "\n",
    "#URL Binding     \n",
    "@app.route('/predict/', methods=['GET'])\n",
    "def predict():\n",
    "    # read parameters from the URL\n",
    "    parameters = request.args.to_dict()\n",
    "    i15 = float(parameters['rainFall'])\n",
    "    siteId = int(parameters['siteId'])\n",
    "    \n",
    "    #identify the site's record\n",
    "    rec = sdf[sdf['ID']==siteId][[\"PropHM23\",\"dNBR/1000\",'KF']]\n",
    "    \n",
    "    # multiply the peak rainfall intensity ( value is derived from parameters)\n",
    "    rec[\"PropHM23_x_i15\"] = rec[\"PropHM23\"] * i15\n",
    "    rec[\"dNBR_x_i15\"] = rec[\"dNBR/1000\"] * i15\n",
    "    rec[\"KF_x_i15\"] = rec[\"KF\"] * i15\n",
    "\n",
    "    #select the features which are required for model prediction\n",
    "    rec = rec[[\"PropHM23_x_i15\",\"dNBR_x_i15\",\"KF_x_i15\"]]\n",
    "\n",
    "    #predict\n",
    "    y_test_pred = lr_model.predict(rec)   \n",
    "    #return the output\n",
    "    return 'Chance of having debris flow : %s' %y_test_pred[0]\n",
    "                 \n",
    "app.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4d1773-bc49-484a-97a4-730877ff57a2",
   "metadata": {},
   "source": [
    "## Flask App to expose NN model ( two layer) as end point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3696d37a-8802-4f7d-8957-78a462f19263",
   "metadata": {},
   "source": [
    "- After running the below cell, flask app will be running at the ip 127.0.0.1:500  \n",
    "\n",
    "- click on the hyperlink http://127.0.0.1:5000/predictForAllSites/?rainFall=100 to get response from the app. we can change the rainfall parameter to check the values \n",
    "\n",
    "- to stop the app running in jupyter notebook, you can either press \"Interrupt Kernel\" in Kernel from menu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e0756f6-8f4c-498e-9baa-18880bc02816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [03/Jun/2023 18:06:52] \"GET /predictForAllSites/?rainFall=100 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Jun/2023 18:06:59] \"GET /predictForAllSites/?rainFall=100 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [03/Jun/2023 18:07:04] \"GET /predictForAllSites/?rainFall=10 HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../model/\")\n",
    "from flask import Flask,request\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import ModelNN as Net\n",
    "import torch\n",
    "app = Flask(__name__)\n",
    "\n",
    "#intialize values\n",
    "seed=27\n",
    "data_dir = '../../data/'\n",
    "model_dir = '../../model/'\n",
    "# nn_model = 'two_layer'\n",
    "nn_model = 'TwoLayer_750_epochs_optimized_roc_auc_score'\n",
    "peak_mmh = 'peak_i15_mmh'\n",
    "\n",
    "# data_file = data_dir + 'data_v09_consolidated.parquet'\n",
    "data_file = data_dir + 'data_v08_consolidated.parquet'\n",
    "\n",
    "#read parameters from config file\n",
    "with open(model_dir + \"model_parameters.json\",\"r\") as jsonfile:\n",
    "    params= json.load(jsonfile)[nn_model]\n",
    "\n",
    "#read model data from file \n",
    "X_train_df = pd.read_parquet(data_file)\n",
    "y_train_df = X_train_df['response']\n",
    "\n",
    "#select only required columns/features\n",
    "nn_data = X_train_df[params['features']]    \n",
    "col_indx = nn_data.columns.get_loc(peak_mmh)\n",
    "\n",
    "#scale data \n",
    "ss = StandardScaler()\n",
    "nn_data = ss.fit_transform(nn_data)\n",
    "\n",
    "#now get the mean and std dev for rain internsity parameter ( from already fitted StandardScaler class)\n",
    "i15_feature_indx = ss.feature_names_in_.tolist().index(peak_mmh)\n",
    "i15_mean= ss.mean_.tolist()[i15_feature_indx]\n",
    "i15_std = np.sqrt(ss.var_.tolist()[i15_feature_indx])\n",
    "\n",
    "#intialize model params\n",
    "input_size = nn_data.shape[1]\n",
    "hidden_size = params['hidden_size']\n",
    "learning_rate = params['lr'] \n",
    "dropout_rate = params['dropout_rate']\n",
    "output_size = 1 \n",
    "\n",
    "#intialize model architechure\n",
    "model = Net.TwoLayer(input_size, hidden_size, output_size, dropout_rate)\n",
    "\n",
    "#load weights\n",
    "\n",
    "# model.load_state_dict(torch.load(model_dir+params[\"weights\"]))\n",
    "if torch.cuda.is_available():\n",
    "    model.load_state_dict(torch.load(model_dir+params[\"weights\"]))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(model_dir+params[\"weights\"],map_location=torch.device('cpu')))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "#URL Binding     \n",
    "@app.route('/predictForAllSites/', methods=['GET'])\n",
    "def predictForAllSites():\n",
    "    \n",
    "    # read parameters from the URL\n",
    "    parameters = request.args.to_dict()\n",
    "    i15 = float(parameters['rainFall'])\n",
    "    \n",
    "    #scale the rainFall parameter\n",
    "    peak_i15_scaled = (i15 - i15_mean)/i15_std\n",
    "    \n",
    "    #update the data with peak_i15_scaled value for all sites\n",
    "    #we are predicting the DF across all sites for given amount of storm rainfall\n",
    "    nn_data[:,col_indx] = peak_i15_scaled\n",
    "    \n",
    "    tensor_data = torch.from_numpy(nn_data).float()\n",
    "\n",
    "    #use model to predict\n",
    "    y_pred = model(tensor_data)\n",
    "    \n",
    "    #convert y_pred tensor to list \n",
    "    df_prob_list =  [np.round(i[0],2) for i in y_pred.detach().numpy().tolist()]\n",
    "    \n",
    "    #return the list of probabilities\n",
    "    return df_prob_list\n",
    "\n",
    "app.run()\n",
    "\n",
    "# \"\"\"\n",
    "# test the process by uncommenting below code\n",
    "# check the train accuracy by loading model and predicting ( No Training)\n",
    "# \"\"\"\n",
    "\n",
    "# #convert to tensors\n",
    "# nn_data = torch.from_numpy(nn_data).float()\n",
    "\n",
    "# # #predict \n",
    "# y_pred = model(nn_data)\n",
    "\n",
    "# # check accuracy\n",
    "# y_train =y_train_df\n",
    "# train_pred_correct = sum(X_train_df['response'] == np.round(y_pred.detach().numpy().flatten()))\n",
    "# train_accuracy = (train_pred_correct / y_train_df.shape[0])\n",
    "# print(train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ee0835-7396-4eb6-b3a3-4bfc24aaf193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "582f6544-89a0-4d78-bb54-26d28b79c052",
   "metadata": {},
   "source": [
    "## Scrath pad ( ignore below cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "668b1cff-15a4-4996-b647-8b7e04495fa2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_15' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     data \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m----> 5\u001b[0m     X \u001b[38;5;241m=\u001b[39m data[\u001b[43mfeatures_15\u001b[49m]\n\u001b[1;32m      6\u001b[0m     y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m     X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mseed)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'features_15' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Load data\n",
    "    data = df.copy()\n",
    "    \n",
    "    X = data[features_15]\n",
    "    y = data['response']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    \n",
    "    #scale the data X_train and X_test\n",
    "    cols = X_train.columns\n",
    "    sc = StandardScaler()\n",
    "    X_train = pd.DataFrame(sc.fit_transform(X_train), columns=cols)\n",
    "    X_test = pd.DataFrame(sc.transform(X_test), columns=cols)\n",
    "    \n",
    "    X_train = torch.tensor(X_train.values).float()\n",
    "    y_train = torch.tensor(y_train.values).float().view(-1, 1)\n",
    "    X_test = torch.tensor(X_test.values).float()\n",
    "    y_test = torch.tensor(y_test.values).float().view(-1, 1)\n",
    "    \n",
    "    input_size = X_train.shape[1]\n",
    "    hidden_size = 500\n",
    "    output_size = 1\n",
    "    dropout_rate = 0.2\n",
    "    learning_rate = 0.001 # 0.001 is default value for Adam optimizer\n",
    "    \n",
    "    model = Net(input_size, hidden_size, output_size, dropout_rate)\n",
    "    model.load_state_dict(torch.load('../../data/model.pth'))\n",
    "\n",
    "    # now final outputs\n",
    "    y_train_pred = model(X_train)\n",
    "    #y_train_prob = torch.sigmoid(y_train_pred) # already a probability\n",
    "\n",
    "    y_test_pred = model(X_test)\n",
    "    # y_test_prob = torch.sigmoid(y_test_pred) # already a probability\n",
    "\n",
    "\n",
    "    # print(\"\\n\")\n",
    "\n",
    "    # accuracy\n",
    "    train_pred_correct = sum(y_train.detach().numpy() == np.round(y_train_pred.detach().numpy()))\n",
    "    train_accuracy = (train_pred_correct / y_train.shape[0])[0]\n",
    "    print(f'Training accuracy: {train_accuracy}')\n",
    "\n",
    "    test_pred_correct = sum(y_test.detach().numpy() == np.round(y_test_pred.detach().numpy()))\n",
    "    test_accuracy = (test_pred_correct / y_test.shape[0])[0]\n",
    "    print(f'Test accuracy: {test_accuracy}')\n",
    "\n",
    "    # # f1\n",
    "    f1_output = f1_score(y_test, np.round(y_test_pred.detach().numpy()))\n",
    "    print(\"\\n\")\n",
    "    print(f'F1 Score (test): {f1_output}')\n",
    "\n",
    "\n",
    "    # # extract AUC for printing\n",
    "    auc_test = roc_auc_score(\n",
    "        y_test.detach().numpy(), \n",
    "        y_test_pred.detach().numpy()\n",
    "    )\n",
    "    print(f'AUC (test): {auc_test}')\n",
    "    print('\\n')\n",
    "\n",
    "# Training loss: 0.023463357239961624\n",
    "# Validation loss: 2.032917022705078\n",
    "\n",
    "\n",
    "# Training accuracy: 0.9908151549942594\n",
    "# Test accuracy: 0.8256880733944955\n",
    "\n",
    "\n",
    "# F1 Score (test): 0.6666666666666666\n",
    "# AUC (test): 0.8762721555174386"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e3ad9a8d-841d-4bac-80d0-019a30baa737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8018539976825029\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../model/\")\n",
    "from flask import Flask,request\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import ModelNN as Net\n",
    "import torch\n",
    "app = Flask(__name__)\n",
    "\n",
    "#intialize values\n",
    "seed=27\n",
    "data_dir = '../../data/'\n",
    "model_dir = '../../model/'\n",
    "nn_model = 'two_layer'\n",
    "peak_mmh = 'peak_i15_mmh'\n",
    "\n",
    "data_file = data_dir + 'data_v09_feature_consolidation.parquet'\n",
    "data_file_pkl = data_dir+ 'train_test_data.pkl'\n",
    "\n",
    "#read parameters from config file\n",
    "with open(model_dir + \"model_parameters.json\",\"r\") as jsonfile:\n",
    "    params= json.load(jsonfile)[nn_model]\n",
    "\n",
    "#read model data from file \n",
    "X_train_df, X_test_df, y_train_df, y_test_df = pickle.load(open(data_file_pkl, \"rb\"))\n",
    "nn_data = X_train_df[params['features']]    \n",
    "col_indx = nn_data.columns.get_loc(peak_mmh)\n",
    "\n",
    "#scale data \n",
    "ss = StandardScaler()\n",
    "nn_data = ss.fit_transform(nn_data)\n",
    "\n",
    "#not get the mean and std dev for rain internsity parameter\n",
    "i15_feature_indx = ss.feature_names_in_.tolist().index(peak_mmh)\n",
    "i15_mean= ss.mean_.tolist()[i15_feature_indx]\n",
    "i15_std = np.sqrt(ss.var_.tolist()[i15_feature_indx])\n",
    "\n",
    "#intialize model params\n",
    "input_size = nn_data.shape[1]\n",
    "hidden_size = params['hidden_size']\n",
    "learning_rate = params['lr'] \n",
    "dropout_rate = params['dropout_rate']\n",
    "output_size = 1 \n",
    "\n",
    "#intialize model architechure\n",
    "model = Net.TwoLayer(input_size, hidden_size, output_size, dropout_rate)\n",
    "\n",
    "#load weights\n",
    "model.load_state_dict(torch.load(model_dir+params[\"weights\"]))\n",
    "model.eval()\n",
    "# #URL Binding     \n",
    "# @app.route('/predictForAllSites/', methods=['GET'])\n",
    "# def predictForAllSites():\n",
    "    \n",
    "#     # read parameters from the URL\n",
    "#     parameters = request.args.to_dict()\n",
    "#     i15 = float(parameters['rainFall'])\n",
    "    \n",
    "#     #scale the rainFall parameter\n",
    "#     peak_i15_scaled = (i15 - i15_mean)/i15_std\n",
    "    \n",
    "#     #update the data with peak_i15_scaled value for all sites\n",
    "#     #we are predicting the DF across all sites for given amount of storm rainfall\n",
    "#     nn_data[:,col_indx] = peak_i15_scaled\n",
    "#     tensor_data = torch.from_numpy(nn_data).float()\n",
    "    \n",
    "#     #use model to predict\n",
    "#     y_pred = model(tensor_data)\n",
    "    \n",
    "#     #convert y_pred tensor to list \n",
    "#     df_prob_list =  [np.round(i[0],2) for i in y_pred.detach().numpy().tolist()]\n",
    "    \n",
    "#     #return the list of probabilities\n",
    "#     return df_prob_list[15:30]\n",
    "\n",
    "# app.run()\n",
    "\n",
    "# \"\"\"\n",
    "# test the process by uncommenting below code\n",
    "# check the train accuracy by loading model and predicting ( No Training)\n",
    "# \"\"\"\n",
    "\n",
    "#convert to tensors\n",
    "nn_data = torch.from_numpy(nn_data).float()\n",
    "\n",
    "# #predict \n",
    "y_pred = model(nn_data)\n",
    "\n",
    "# check accuracy\n",
    "y_train = torch.tensor(y_train_df.values).float().view(-1, 1)\n",
    "train_pred_correct = sum( y_train.detach().numpy() == np.round(y_pred.detach().numpy()))\n",
    "train_accuracy = (train_pred_correct / y_train.shape[0])[0]\n",
    "print(train_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43df8cca-00f7-46e5-af25-36923607ff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries ( add new ones to requirments.txt)\n",
    "\n",
    "# load data file \n",
    "\n",
    "# scale the data(all columns)\n",
    "\n",
    "# get the mean and std. dev for rainfall param ( check if we get this value from scaler)\n",
    "\n",
    "# scale the value passed in by UI which is rainfall parameter\n",
    "\n",
    "# update the scaled data df , with new scaled rainfall ( same value for all recs) \n",
    "\n",
    "\n",
    "# read the model from NN-Model-Class.py\n",
    "\n",
    "# fit the model from .pth file \n",
    "\n",
    "# predict the debrisflow for all sites in the scaled data file \n",
    "\n",
    "\n",
    "# does the UI , process one site at a time ?\n",
    "\n",
    "\n",
    "# there are 600/1000 unique sites, we have 600 records in the test set? (assumption: one call )\n",
    "\n",
    "\n",
    "# can the flask api return the pandas df  or np.array with site-id and response variable ?\n",
    "# or does it have to another way? \n",
    "\n",
    "# does the nn-model does return 0 or 0\n",
    "\n",
    "    \"\"\"column has one static name \n",
    "    \n",
    "    1.Input rain ( unsclaed value) ; 2.DBF_Prob (0-1)\n",
    "    \n",
    "    2.600 rec pandas df \n",
    "    \n",
    "    3.visualizaiton is looking at column 2 ; seeded at ( prob as-is)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_debris_flow",
   "language": "python",
   "name": "venv_debris_flow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
